{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzfBmBwXEYlIWbHuHMMPMq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomeiLam/langchain-example/blob/main/LangChain_chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KzhldLY0q5U"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install langchain_community\n",
        "!pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "MqaZjj4e00u1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "2qp3TkV61TOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Data.csv')"
      ],
      "metadata": {
        "id": "HgUER0c81UMs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pefFtf2g1wo_",
        "outputId": "86328a0f-11d4-4051-8cd7-4601eff5b925"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Product                                             Review\n",
              "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
              "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
              "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
              "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
              "4  Milk Frother Handheld\\n  Â I loved this product. But they only seem to l..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-551894e3-8904-42ab-9be7-8997e6a015ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen Size Sheet Set</td>\n",
              "      <td>I ordered a king size set. My only criticism w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Waterproof Phone Pouch</td>\n",
              "      <td>I loved the waterproof sac, although the openi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luxury Air Mattress</td>\n",
              "      <td>This mattress had a small hole in the top of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pillows Insert</td>\n",
              "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milk Frother Handheld\\n</td>\n",
              "      <td>I loved this product. But they only seem to l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-551894e3-8904-42ab-9be7-8997e6a015ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-551894e3-8904-42ab-9be7-8997e6a015ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-551894e3-8904-42ab-9be7-8997e6a015ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-13013e0c-b998-4c5b-ae1e-5c6e960a741c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13013e0c-b998-4c5b-ae1e-5c6e960a741c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-13013e0c-b998-4c5b-ae1e-5c6e960a741c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Queen Size Sheet Set\",\n          \"Waterproof Phone Pouch\",\n          \"L'Or Espresso Caf\\u00e9\\u00a0\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"I ordered a king size set. My only criticism would be that I wish seller would offer the king size set with 4 pillowcases. I separately ordered a two pack of pillowcases so I could have a total of four. When I saw the two packages, it looked like the color did not exactly match. Customer service was excellent about sending me two more pillowcases so I would have four that matched. Excellent! For the cost of these sheets, I am satisfied with the characteristics and coolness of the sheets.\",\n          \"I loved the waterproof sac, although the opening was made of a hard plastic. I don\\u2019t know if that would break easily. But I couldn\\u2019t turn my phone on, once it was in the pouch.\",\n          \"Je trouve le go\\u00fbt m\\u00e9diocre. La mousse ne tient pas, c'est bizarre. J'ach\\u00e8te les m\\u00eames dans le commerce et le go\\u00fbt est bien meilleur...\\nVieux lot ou contrefa\\u00e7on !?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "kzcrnelu1ywl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain's LLMChain has been deprecate as of version 0.1.17 in favor of the \"Runnable\" API. Instead of import LLMChain from langchain.chains, we now compose prompts and models as runnables, chaining them with the pipe operator (|) or via `RunnableSequence`. We can migrate it as:"
      ],
      "metadata": {
        "id": "6fpj5dZi6NvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableSequence, RunnableLambda"
      ],
      "metadata": {
        "id": "b7HHpVvh6jsI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is RunnableSequence?\n",
        "A `RunnableSequence` is the core composition primitive in LangChainâs new Runnable API. It implements the `Runnable` interface and simply runs a series of smaller ârunnablesâ one after the other, piping the output of each into the next. Under the hood:\n",
        "\n",
        "* It implements the standard methodsâ`.invoke()` / `.ainvoke()`, `.batch()` / `.abatch()`, and streaming variantsâso you get sync, async, batch, and streaming support âfor free.â\n",
        "\n",
        "* You can construct one explicitly:\n",
        "```python\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "chain = RunnableSequence(first=prompt, middle=[], last=llm)\n",
        "```\n",
        "* Most commonly, you build a RunnableSequence with the pipe operator (|), which is syntactic sugar for the same thing:\n",
        "```python\n",
        "chain = prompt | llm\n",
        "```\n",
        "Under the hood it is exactly equivalent to:\n",
        "```python\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "chain = RunnableSequence(prompt, llm)\n",
        "```\n",
        "Hereâs what happens stepâbyâstep:\n",
        "\n",
        "1. prompt (a ChatPromptTemplate) is a Runnable that, given {\"product\": \"X\"}, produces a list of chat messages (the filledâin prompt).\n",
        "\n",
        "2. llm (a ChatOpenAI) is a Runnable that, given a list of chat messages, sends them to the model and returns a chat response.\n",
        "\n",
        "3. Chaining with | wires the output of the left Runnable into the input of the right Runnable, producing a new RunnableSequence.\n",
        "\n",
        "You get the same behaviorâand identical `.invoke()` interfaceâbut with far more readable, âpipeâstyleâ syntax. You can chain as many steps as you like:\n",
        "```python\n",
        "# e.g. format â generate â parse â store\n",
        "chain = prompt | llm | MyParserRunnable() | MyStorageRunnable()\n",
        "```\n",
        "\n",
        "This wires the promptâs output into the LLMâs input, creating a single composite runnable\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Because itâs itself a Runnable, you can treat the entire chain just like any other primitive:\n",
        "```python\n",
        "# Synchronous\n",
        "response = (prompt | llm).invoke({\"product\": \"eco-friendly sneakers\"})\n",
        "print(response.content)\n",
        "\n",
        "# Asynchronous\n",
        "response = await (prompt | llm).ainvoke({\"product\": \"eco-friendly sneakers\"})\n",
        "```\n",
        "\n",
        "Behind the scenes `RunnableSequence` handles validation, batching, and streaming. Itâs the building block that replaces `LLMChain`, giving you much more flexibility and composability.\n"
      ],
      "metadata": {
        "id": "0wtfoSNw83NF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the LLM and prompt:"
      ],
      "metadata": {
        "id": "3jGl_WLS6rYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatOpenAI is a Runnable for OpenAIâs chat models\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9)\n",
        "\n",
        "# ChatPromptTemplate.from_template yields a Runnable prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe a company that makes {product}?\"\n",
        ")"
      ],
      "metadata": {
        "id": "xDUYLYP66nYa"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain them together:"
      ],
      "metadata": {
        "id": "TFqugtAg7FJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: using the pipe operator\n",
        "chain = prompt | llm\n",
        "\n",
        "# Option B: explicitly building a RunnableSequence\n",
        "chain = RunnableSequence(prompt, llm)"
      ],
      "metadata": {
        "id": "kaGUurf52buV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the chain:"
      ],
      "metadata": {
        "id": "d37tldlq7lBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the full invoke output\n",
        "output = chain.invoke({\"product\": \"Queen Size Sheet Set\"})\n",
        "print(output)  # e.g. {'text': 'Greensole', ...}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IRX1cjw7nD6",
        "outputId": "80eeb7d3-2daa-4f80-9975-592c1c2a4380"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Royal Comfort Sheets' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 23, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-6ca68340-d7cb-468a-ae10-df27501e80a0-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimpleSequentialChain"
      ],
      "metadata": {
        "id": "WR3uU972PsBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ],
      "metadata": {
        "id": "fcI1rHlBPtI6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9)\n",
        "\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = RunnableSequence(\n",
        "    first_prompt,\n",
        "    llm,\n",
        "    RunnableLambda(\n",
        "      lambda msg: {\"company_name\": msg.content}\n",
        "    ))\n",
        "\n",
        "# Option 2: use the pipe operator (syntactic sugar)\n",
        "chain_one = first_prompt | llm | RunnableLambda(\n",
        "    lambda msg: {\"company_name\": msg.content})"
      ],
      "metadata": {
        "id": "GAR9rpHNPxfW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = RunnableSequence(second_prompt, llm)\n",
        "\n",
        "# Option 2: use the pipe operator (syntactic sugar)\n",
        "chainchain_two_one = second_prompt | llm"
      ],
      "metadata": {
        "id": "JaeuROssPzNR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product = \"Queen Size Sheet Set\"\n",
        "#extract_company = RunnableLambda(lambda msg: {\"company_name\": msg.content})\n",
        "overall_simple_chain = RunnableSequence(\n",
        "    chain_one,\n",
        "    # extract_company,\n",
        "    chain_two,\n",
        ")"
      ],
      "metadata": {
        "id": "e48PFh68Q5Fw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = overall_simple_chain.invoke({\"product\": \"Queen Size Sheet Set\"})\n",
        "# if the prompt template only defines a single input variable, we can pass that\n",
        "# value directly to .invoke(...)\n",
        "# overall_simple_chain.invoke(\"Queen Size Sheet Set\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLfGaNEPRft8",
        "outputId": "a39b3509-dde9-40d3-c64d-1359959b3479"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Royal Rest Bedding Co. offers luxurious, comfortable mattresses and bedding products fit for royalty, guaranteeing a good night's sleep.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SequentialChain"
      ],
      "metadata": {
        "id": "JRCiY0oOUbj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "YUw7ALXoUcx2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9)\n",
        "\n",
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = RunnableSequence(first_prompt, llm)"
      ],
      "metadata": {
        "id": "3tZxFkqKUfnS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = RunnableSequence(second_prompt, llm)"
      ],
      "metadata": {
        "id": "soVMhkKCUpnY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = RunnableSequence(third_prompt, llm)"
      ],
      "metadata": {
        "id": "GoQ1j7QeU1JA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = RunnableSequence(fourth_prompt, llm)\n"
      ],
      "metadata": {
        "id": "5YqyKCrwV9qa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lambda func passing into `RunnableLambda` is a oneâstep âcontext mergerâ that:\n",
        "\n",
        "1. Takes the current context dict as its sole argument, which weâre calling ctx.\n",
        "\n",
        "2. Unpacks every key/value pair from that dict into a brandânew dict via **ctx.\n",
        "\n",
        "3. Adds one more entry under the key \"English_Review\", whose value is computed by running your translation chain on the original review text (ctx[\"Review\"]) and pulling out its .content.\n",
        "\n",
        "```python\n",
        "lambda ctx: {\n",
        "  **ctx, # copy in everything already in ctx\n",
        "  \"English_Review\": chain_one.invoke(ctx[\"Review\"]).content\n",
        "}\n",
        "```\n",
        "* `ctx[\"Review\"]` fetches the raw review string.\n",
        "\n",
        "* `chain_one.invoke(...)` runs your first promptâLLM pipeline and returns a BaseMessage.\n",
        "\n",
        "* `.content` extracts the generated string from that message.\n",
        "\n",
        "* The resulting dict now has every key that was in `ctx` plus `\"English_Review\": \"<translated text>\"`.\n"
      ],
      "metadata": {
        "id": "LNZrXPxBadt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "\n",
        "# A) seed the context with the raw Review string\n",
        "seed = RunnableLambda(lambda review: {\"Review\": review})\n",
        "\n",
        "# B) run translation, append \"English_Review\" but keep full context\n",
        "chain_one_ctx = RunnableLambda(\n",
        "    lambda ctx: {\n",
        "        **ctx,\n",
        "        \"English_Review\": chain_one.invoke(ctx[\"Review\"]).content\n",
        "    }\n",
        ")\n",
        "\n",
        "# C) run summarization, append \"summary\"\n",
        "chain_two_ctx = RunnableLambda(\n",
        "    lambda ctx: {\n",
        "        **ctx,\n",
        "        \"summary\": chain_two.invoke(ctx[\"English_Review\"]).content\n",
        "    }\n",
        ")\n",
        "\n",
        "# D) inject your desired language for the followâup\n",
        "#    (you could also parameterize this or read from ctx, but here itâs hardâcoded)\n",
        "chain_three_ctx = RunnableLambda(\n",
        "    lambda ctx: {\n",
        "        **ctx,\n",
        "        \"language\": chain_three.invoke(ctx[\"Review\"]).content\n",
        "    }\n",
        ")\n",
        "\n",
        "# E) run followâup, append \"followup_message\"\n",
        "chain_four_ctx = RunnableLambda(\n",
        "    lambda ctx: {\n",
        "        **ctx,\n",
        "        \"followup_message\": chain_four.invoke({\n",
        "            \"summary\":  ctx[\"summary\"],\n",
        "            \"language\": ctx[\"language\"]\n",
        "        }).content\n",
        "    }\n",
        ")\n",
        "\n",
        "# Finally stitch them all together:\n",
        "overall_chain = RunnableSequence(\n",
        "    seed,\n",
        "    chain_one_ctx,\n",
        "    chain_two_ctx,\n",
        "    chain_three_ctx,\n",
        "    chain_four_ctx,\n",
        ")"
      ],
      "metadata": {
        "id": "xXAgjO-_WDHV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.Review[5]\n",
        "review\n",
        "overall_chain.invoke(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKP_N5JNWhZH",
        "outputId": "fc28b466-55ef-4d3d-839e-f80b55499ba7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goÃ»t mÃ©diocre. La mousse ne tient pas, c'est bizarre. J'achÃ¨te les mÃªmes dans le commerce et le goÃ»t est bien meilleur...\\nVieux lot ou contrefaÃ§on !?\",\n",
              " 'English_Review': \"I find the taste poor. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer is dissatisfied with the poor taste and quality of the product, suspecting that it may be an old batch or counterfeit.',\n",
              " 'language': 'French',\n",
              " 'followup_message': \"Cher client,\\n\\nNous sommes profondÃ©ment dÃ©solÃ©s d'apprendre que vous n'Ãªtes pas satisfait de votre rÃ©cente expÃ©rience avec notre produit. Nous prenons vos prÃ©occupations trÃ¨s au sÃ©rieux et nous voulons vous assurer que la qualitÃ© de nos produits est notre prioritÃ© absolue.\\n\\nNous comprenons vos inquiÃ©tudes concernant la possibilitÃ© d'un lot pÃ©rimÃ© ou contrefait et nous prenons des mesures immÃ©diates pour enquÃªter sur la situation. Si vous le souhaitez, nous serions ravis de vous offrir un remboursement ou un remplacement pour compenser votre dÃ©ception.\\n\\nNous vous remercions de votre retour d'expÃ©rience et de votre patience pendant que nous travaillons Ã  rÃ©soudre ce problÃ¨me. Votre satisfaction est trÃ¨s importante pour nous, et nous ferons tout notre possible pour rectifier la situation.\\n\\nCordialement,\\nL'Ã©quipe du service client\"}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Router Chain"
      ],
      "metadata": {
        "id": "WFPy9tf0ba3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "1QI_qLwvbel8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "a0JWo3d5mXFS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableSequence"
      ],
      "metadata": {
        "id": "eyI9C0HfmuUf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9)"
      ],
      "metadata": {
        "id": "5FQYEbSmmwY3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = RunnableSequence(prompt, llm)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "X21u09lom6vj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = RunnableSequence(default_prompt, llm)"
      ],
      "metadata": {
        "id": "6ZdfStgJnwkU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: The value of âdestinationâ MUST match one of \\\n",
        "the candidate prompts listed below.\\\n",
        "If âdestinationâ does not fit any of the specified prompts, set it to âDEFAULT.â\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "Us5ymqvRn5-8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "x6hO5M9foPma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_for_router = RunnableLambda(lambda d: {\n",
        "    \"key\":   d[\"destination\"],\n",
        "    \"input\": d[\"next_inputs\"],\n",
        "})\n",
        "map_for_router"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1332fniSoZYr",
        "outputId": "551e185f-43e1-43f1-8393-626df1ce1e52"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableLambda(lambda d: {'key': d['destination'], 'input': d['next_inputs']})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.router import RouterRunnable\n",
        "\n",
        "router = RouterRunnable(\n",
        "    runnables={\n",
        "        **destination_chains,       # your named branches\n",
        "        \"DEFAULT\": default_chain,   # the fallback branch\n",
        "    }\n",
        ")\n",
        "\n",
        "overall_chain = RunnableSequence(\n",
        "    router_chain,    # picks {\"destination\",\"next_inputs\"}\n",
        "    RunnableLambda(lambda d: {\"key\": d[\"destination\"], \"input\": d[\"next_inputs\"]}),\n",
        "    router,          # runs only runnables[key], falling back to \"DEFAULT\"\n",
        ")\n",
        "result = overall_chain.invoke({\"input\": \"Tell me about the Pythagorean theorem.\"})\n",
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "ajf3OyoHqxgK",
        "outputId": "0b8433af-35b4-4228-d35a-8e357c14895c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Pythagorean theorem is a fundamental principle in geometry that states that in a right-angled triangle, the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the lengths of the other two sides. In equation form, it is written as: \\n\\na^2 + b^2 = c^2\\n\\nWhere 'a' and 'b' are the lengths of the two shorter sides of the triangle, and 'c' is the length of the hypotenuse. This theorem is named after the ancient Greek mathematician Pythagoras, although it is believed that it was known to the Babylonians even earlier.\\n\\nThe Pythagorean theorem has many practical applications in various fields such as physics, engineering, and architecture. It is used to calculate distances, solve equations involving right triangles, and even in the development of computer graphics algorithms. It is one of the most well-known and widely used mathematical principles in the world.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap_branch(chain, key_name):\n",
        "    # Takes {\"input\":...} â runs chain â returns {\"destination\":key_name, \"response\": <text>}\n",
        "    return RunnableSequence(\n",
        "        chain,  # expects a single string input or {\"input\": ...}\n",
        "        RunnableLambda(lambda msg, *, key=key_name: {\n",
        "            \"destination\": key,\n",
        "            \"response\": msg.content\n",
        "        })\n",
        "    )\n",
        "    # wrap all your branches (and default) under the same dict keys\n",
        "wrapped = {\n",
        "    **{k: wrap_branch(c, k) for k, c in destination_chains.items()},\n",
        "    \"DEFAULT\": wrap_branch(default_chain, \"DEFAULT\"),\n",
        "}\n",
        "\n",
        "router = RouterRunnable(runnables=wrapped)\n",
        "\n",
        "overall = RunnableSequence(\n",
        "    # step A: pick destination & inputs\n",
        "    router_chain,\n",
        "    # step B: reshape into {\"key\",\"input\"} for RouterRunnable\n",
        "    RunnableLambda(lambda d: {\"key\": d[\"destination\"], \"input\": d[\"next_inputs\"]}),\n",
        "    # step C: run only the chosen wrapped branch\n",
        "    router,\n",
        ")\n",
        "\n",
        "out = overall.invoke(\"How do I train a neural network?\")\n",
        "# out == {\"destination\": \"computer science\", \"response\": \"You train byâ¦\"}\n",
        "print(\"Chose branch:\", out[\"destination\"])\n",
        "print(\"Got response:\", out[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ88GoD_rskj",
        "outputId": "8d19e4ce-5a68-4c99-b423-937320ace6da"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chose branch: computer science\n",
            "Got response: Training a neural network involves several steps:\n",
            "\n",
            "1. Data collection: Gather a dataset that includes input data and corresponding output data.\n",
            "\n",
            "2. Data preprocessing: Clean and preprocess the data to ensure it is in a suitable format for training the neural network.\n",
            "\n",
            "3. Choose a neural network architecture: Decide on the number of layers, type of activation functions, and other parameters for the neural network.\n",
            "\n",
            "4. Initialize the weights and biases: Randomly initialize the weights and biases for the neural network.\n",
            "\n",
            "5. Forward propagation: Pass input data through the neural network to compute the output.\n",
            "\n",
            "6. Calculate the loss: Compare the output of the neural network with the actual output and calculate the loss using a loss function.\n",
            "\n",
            "7. Backward propagation (backpropagation): Adjust the weights and biases of the neural network to minimize the loss using gradient descent or other optimization algorithms.\n",
            "\n",
            "8. Update the weights and biases: Update the weights and biases of the neural network based on the gradients computed during backpropagation.\n",
            "\n",
            "9. Repeat steps 5-8: Iterate through the dataset multiple times (epochs) to train the neural network and minimize the loss.\n",
            "\n",
            "10. Evaluate the performance: Test the trained neural network on a separate testing dataset to evaluate its performance and make any necessary adjustments.\n",
            "\n",
            "By following these steps, you can effectively train a neural network to perform a specific task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RouterRunnable inside a RunnableSequence\n",
        "\n",
        "When we need our application to choose among multiple ânext stepsââfor example, different LLM prompts or modelsâbased on the userâs input, you can use LangChainâs `RouterRunnable` inside a `RunnableSequence` to build a single, composable pipeline:\n",
        "\n",
        "### 1. Core concepts\n",
        "* **RunnableSequence:**\n",
        "A composable pipeline of one or more ârunnablesâ (prompts, models, lambdas, routers). It takes the output of each step and feeds it into the next, supporting sync (`.invoke()`), async (`.ainvoke()`), batch, and streaming APIs uniformly.\n",
        "\n",
        "* **RouterRunnable:**\n",
        "A special runnable that, given a dict with `{\"key\": <branchâname>, \"input\": <userâtext>}`, looks up `self.runnables[key]` in its internal mapping and runs only that one branchâs runnable on the given input. If the key isnât found, it falls back to the \"DEFAULT\" branch.\n",
        "\n",
        "* **Mapping step:**\n",
        "Before calling a `RouterRunnable`, we must reshape our routerâs output (often from an `LLMRouterChain`) into the exact `{ \"key\": ..., \"input\": ... }` shape the router expects. A tiny `RunnableLambda` does the trick.\n",
        "\n",
        "### 2. Step-by-step breakdown\n",
        "1. **Route** -\n",
        "We run an LLMRouterChain (or any model) with a âmultiâprompt routerâ template. Its output is a dict like:\n",
        "```python\n",
        "{\n",
        "  \"destination\": \"math\",\n",
        "  \"next_inputs\": \"Compute the derivative of x^2\"\n",
        "}\n",
        "```\n",
        "\n",
        "2. **Reshape** -\n",
        "Convert that into the routerâs format:\n",
        "```python\n",
        "{\"key\": destination, \"input\": next_inputs}\n",
        "```\n",
        "via:\n",
        "```python\n",
        "map_for_router = RunnableLambda(lambda d: {\n",
        "    \"key\":   d[\"destination\"],\n",
        "    \"input\": d[\"next_inputs\"],\n",
        "})\n",
        "```\n",
        "\n",
        "3. **Dispatch** -\n",
        "A single `RouterRunnable` is constructed with a mapping of all our branches (plus a `\"DEFAULT\"` fallback):\n",
        "```python\n",
        "router = RouterRunnable(runnables={\n",
        "  \"physics\":     physics_chain,\n",
        "  \"math\":        math_chain,\n",
        "  \"history\":     history_chain,\n",
        "  \"computer science\": cs_chain,\n",
        "  \"DEFAULT\":     default_chain\n",
        "})\n",
        "```\n",
        "\n",
        "4. **Compose** -\n",
        "Put it all inside one RunnableSequence so we can call it in one shot:\n",
        "```python\n",
        "overall = RunnableSequence(\n",
        "  router_chain,    # Step A: pick branch & next_inputs\n",
        "  map_for_router,  # Step B: reshape to {\"key\",\"input\"}\n",
        "  router,          # Step C: run only the chosen branch\n",
        ")\n",
        "```\n",
        "\n",
        "5. **Invoke** -\n",
        "```python\n",
        "# Synchronous\n",
        "result = overall.invoke({\"input\": \"How do I solve integrals?\"})\n",
        "print(result.content)  # e.g. the math_chainâs answer\n",
        "# Asynchronous\n",
        "# result = await overall.ainvoke(\"What caused the fall of Rome?\")\n",
        "```\n",
        "\n",
        "### 3. Full example code\n",
        "```python\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableLambda\n",
        "from langchain_core.runnables.router import RouterRunnable\n",
        "from langchain.chains.router import LLMRouterChain\n",
        "\n",
        "# 1) Define your base LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# 2) Build your router prompt & chain\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input...\n",
        "<< CANDIDATE PROMPTS >>\n",
        "physics: for physics questions\n",
        "math:     for math questions\n",
        "history:  for history questions\n",
        "computer science: for CS questions\n",
        "<< INPUT >>\n",
        "{input}\n",
        "\n",
        "<< OUTPUT (JSON)>>\"\"\"\n",
        "router_prompt = ChatPromptTemplate.from_template(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
        "router_chain = LLMRouterChain.from_llm(llm=llm, router_prompt=router_prompt)\n",
        "\n",
        "# 3) Map the routerâs output into {\"key\",\"input\"}\n",
        "map_for_router = RunnableLambda(lambda d: {\n",
        "    \"key\":   d[\"destination\"],\n",
        "    \"input\": d[\"next_inputs\"],\n",
        "})\n",
        "\n",
        "# 4) Define each branch as a prompt|llm (theyâre Runnables too!)\n",
        "destination_chains = {\n",
        "    \"physics\": (ChatPromptTemplate.from_template(\"Physics answer: {input}\") | llm),\n",
        "    \"math\":    (ChatPromptTemplate.from_template(\"Math answer: {input}\")    | llm),\n",
        "    \"history\": (ChatPromptTemplate.from_template(\"History answer: {input}\") | llm),\n",
        "    \"computer science\": (\n",
        "       ChatPromptTemplate.from_template(\"CS answer: {input}\") | llm\n",
        "    ),\n",
        "}\n",
        "\n",
        "# 5) Add a DEFAULT fallback\n",
        "default_chain = ChatPromptTemplate.from_template(\"General: {input}\") | llm\n",
        "\n",
        "# 6) Create the RouterRunnable with all branches\n",
        "router = RouterRunnable(runnables={**destination_chains, \"DEFAULT\": default_chain})\n",
        "\n",
        "# 7) Stitch into one sequence\n",
        "overall_chain = RunnableSequence(\n",
        "    router_chain,    # A: decide branch\n",
        "    map_for_router,  # B: reshape output\n",
        "    router,          # C: dispatch chosen branch\n",
        ")\n",
        "\n",
        "# 8) Run it!\n",
        "query = \"Can you explain Newton's laws?\"\n",
        "response = overall_chain.invoke({\"input\": query})\n",
        "print(\"Branch answer:\", response.content)\n",
        "\n",
        "```\n",
        "\n",
        "### Key points\n",
        "* **Single entrypoint.** Our application only ever calls `overall_chain.invoke()` (or `.ainvoke()`), regardless of how many branches you have.\n",
        "\n",
        "* **Easy to extend.** To add a new topic, just add a new keyârunnable in the `runnables` dict.\n",
        "\n",
        "* **Fully composable.** You can pipe more steps before or afterâlogging, caching, error handling, whateverâsimply by adding more runnables to the `RunnableSequence`."
      ],
      "metadata": {
        "id": "lOtV4z2LuJcS"
      }
    }
  ]
}